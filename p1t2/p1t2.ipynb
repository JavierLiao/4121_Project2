{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fc839e",
   "metadata": {},
   "source": [
    "# Part 1: Spark and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c4df3",
   "metadata": {},
   "source": [
    "## Task2 : Webgraph on Internal Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7e1f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/30 00:38:11 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/04/30 00:38:11 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/04/30 00:38:11 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/30 00:38:11 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "## initial Setup\n",
    "\n",
    "import re \n",
    "import regex\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, lower\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.14.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36467c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/30 00:38:29 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/30 00:38:29 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #0,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## df\n",
    "\n",
    "small = spark.read.format('xml').options(rowTag='page').load('hdfs:/enwiki_small.xml')\n",
    "#test = spark.read.format('xml').options(rowTag='page').load('hdfs:/enwiki_test.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4123a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_internal_link(text):\n",
    "    matches = regex.findall(r'\\[\\[((?:[^[\\]]+|(?R))*+)\\]\\]', text)\n",
    "    result = []\n",
    "    for match in matches:\n",
    "        splited = re.split(r'\\|', match)\n",
    "        temp = []\n",
    "        for link in splited:\n",
    "            if len(re.findall(r'^category:', link)) >0:\n",
    "                temp.append(link)\n",
    "            elif len(re.findall(r':|#', link)) > 0:\n",
    "                continue\n",
    "            else:\n",
    "                temp.append(link)\n",
    "        if len(temp)>0 :\n",
    "            result.append(temp[0]) \n",
    "        else:\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "get_internal_link = udf(get_internal_link, ArrayType(StringType()))\n",
    "\n",
    "## map reduce function\n",
    "def process(df):\n",
    "    output = df.select(lower(df.title).alias('title'), lower(df.revision.text._Value).alias('text')) \\\n",
    "            .dropna(how = 'any') \\\n",
    "            .withColumn('internal_link', get_internal_link('text')) \\\n",
    "            .select('title', explode('internal_link').alias('internal_link')) \\\n",
    "            .sort('title', 'internal_link')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5445b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|           title|       internal_link|\n",
      "+----------------+--------------------+\n",
      "|\"love and theft\"| mississippi (bob...|\n",
      "|\"love and theft\"|                2001|\n",
      "|\"love and theft\"|accidents & accus...|\n",
      "|\"love and theft\"|           accordion|\n",
      "|\"love and theft\"|            allmusic|\n",
      "|\"love and theft\"|   americana (music)|\n",
      "|\"love and theft\"|anthology of amer...|\n",
      "|\"love and theft\"|        augie meyers|\n",
      "|\"love and theft\"|        augie meyers|\n",
      "|\"love and theft\"|               banjo|\n",
      "|\"love and theft\"|         bass guitar|\n",
      "|\"love and theft\"|       billboard 200|\n",
      "|\"love and theft\"|       billboard 200|\n",
      "|\"love and theft\"|  blender (magazine)|\n",
      "|\"love and theft\"|               blues|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# small\n",
    "small_result = process(small)\n",
    "small_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559bb5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:====================================================>    (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|      title|      internal_link|\n",
      "+-----------+-------------------+\n",
      "|  ! (chess)|punctuation (chess)|\n",
      "|!!! (album)|                !!!|\n",
      "|!!! (album)|                !!!|\n",
      "|!!! (album)|              album|\n",
      "|!!! (album)|              album|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_result = process(test).limit(5)\n",
    "test_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7be6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#save samll result to gcs\n",
    "gcs_filepath = 'gs://hw2-backup/p1t2.csv'\n",
    "small_result.limit(5).coalesce(1).write.option(\"delimiter\", \"\\t\").csv(gcs_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}